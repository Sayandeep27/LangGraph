# ğŸš€ Streaming in LangGraph â€” Complete Guide

---

## ğŸ“Œ What is Streaming in LangGraph?

**Streaming in LangGraph** allows you to receive outputs from your graph **step-by-step in real time** instead of waiting for the full execution to finish.

Instead of:

```
User â†’ Graph runs â†’ Final result returned
```

You get:

```
User â†’ Node executes â†’ Partial output â†’ Next node â†’ Partial output â†’ Final output
```

This makes applications:

* Faster
* More interactive
* Transparent
* Suitable for real-time UI updates

---

# ğŸ¯ Why Streaming is Needed

## Without Streaming

* User waits for full execution
* No visibility of intermediate steps
* Slow UX for long LLM calls

## With Streaming

* See responses live
* Track graph execution
* Show progress in UI
* Better debugging
* Real-time agent systems

---

# ğŸ§  Core Idea

LangGraph allows you to stream:

* **State updates**
* **Node outputs**
* **LLM tokens**
* **Events during execution**

Streaming happens using:

```
graph.stream()
```

instead of:

```
graph.invoke()
```

---

# âš¡ invoke vs stream

| Feature       | invoke()          | stream()             |
| ------------- | ----------------- | -------------------- |
| Execution     | Runs fully        | Runs step-by-step    |
| Output        | Final result only | Intermediate + final |
| Real-time     | âŒ No              | âœ… Yes                |
| Debugging     | Hard              | Easy                 |
| UI updates    | âŒ                 | âœ…                    |
| Agent systems | Limited           | Excellent            |

---

# ğŸ—ï¸ Basic Streaming Flow

```
User Input
   â†“
Graph starts
   â†“
Node 1 executes â†’ streamed
   â†“
Node 2 executes â†’ streamed
   â†“
Final output
```

---

# âœ… Basic Streaming Example (Step-by-Step)

## Step 1 â€” Install

```bash
pip install langgraph langchain-openai
```

---

## Step 2 â€” Import Libraries

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END
```

---

## Step 3 â€” Define State

```python
class MyState(TypedDict):
    text: str
```

---

## Step 4 â€” Create Nodes

```python
def node1(state: MyState):
    print("Node 1 running")
    return {"text": state["text"] + " â†’ processed by node1"}


def node2(state: MyState):
    print("Node 2 running")
    return {"text": state["text"] + " â†’ processed by node2"}
```

---

## Step 5 â€” Build Graph

```python
graph = StateGraph(MyState)

graph.add_node("node1", node1)
graph.add_node("node2", node2)

graph.add_edge(START, "node1")
graph.add_edge("node1", "node2")
graph.add_edge("node2", END)

app = graph.compile()
```

---

## Step 6 â€” Stream Execution

```python
for event in app.stream({"text": "hello"}):
    print(event)
```

---

## Output (Step-by-Step)

```
Node 1 running
{'node1': {'text': 'hello â†’ processed by node1'}}

Node 2 running
{'node2': {'text': 'hello â†’ processed by node1 â†’ processed by node2'}}
```

Notice:

âœ” Each node output arrives separately

âœ” You see progress live

âœ” You donâ€™t wait for final result

---

# ğŸ”„ How Streaming Works Internally

When streaming:

1. Graph starts execution
2. Node executes
3. State update emitted
4. Event returned immediately
5. Next node executes

So execution becomes **event-based**.

---

# ğŸ“¦ What stream() Returns

`stream()` returns a **generator** that yields events.

Each event contains:

* Node name
* Updated state
* Execution progress

Example:

```python
{'node_name': {'updated_state'}}
```

---

# ğŸ¯ Streaming Modes in LangGraph

LangGraph supports different streaming modes.

---

## 1ï¸âƒ£ values Mode (Default)

Streams updated state values.

```python
for event in app.stream(input, stream_mode="values"):
    print(event)
```

### What you get

* Updated state after node execution
* Most commonly used mode

---

## 2ï¸âƒ£ updates Mode

Streams only changes made by nodes.

```python
for event in app.stream(input, stream_mode="updates"):
    print(event)
```

### What you get

* Only modified fields
* Efficient for large states

---

## 3ï¸âƒ£ debug Mode

Shows detailed execution events.

```python
for event in app.stream(input, stream_mode="debug"):
    print(event)
```

### What you get

* Node start/end
* State transitions
* Execution trace

Great for debugging.

---

# ğŸ¤– Streaming with LLM Responses

Streaming is extremely useful with LLMs.

Instead of waiting for full response â†’ get tokens live.

---

## Example â€” Chat Streaming

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(streaming=True)


def chat_node(state):
    response = llm.invoke(state["messages"])
    return {"messages": [response]}
```

Then:

```python
for event in app.stream(input):
    print(event)
```

---

# ğŸ¨ Real-World Use Cases

## âœ… Chat applications

* Live message generation

## âœ… AI agents

* Show tool usage step-by-step

## âœ… Debugging workflows

* Trace node execution

## âœ… Monitoring pipelines

* Track progress

## âœ… Web apps

* Real-time UI updates

---

# ğŸ†š invoke() vs stream() vs batch()

| Method     | Purpose                           |
| ---------- | --------------------------------- |
| `invoke()` | Run once and return final result  |
| `stream()` | Run step-by-step with live output |
| `batch()`  | Run multiple inputs together      |

---

# âš ï¸ Important Notes

## 1. stream() is non-blocking

Execution continues while you receive results.

---

## 2. Uses Python generator

You must iterate:

```python
for event in app.stream(input):
```

---

## 3. Best for long workflows

Short workflows may not need streaming.

---

## 4. Useful for UI frameworks

* Streamlit
* FastAPI
* WebSocket apps

---

# ğŸ§© Advanced Pattern â€” Stream + Conditional Routing

Streaming works even when graph has:

* Conditional edges
* Agent loops
* Tool calling

You see each decision step live.

---

# ğŸ”¥ Real Intuition (Very Important)

Think of streaming like:

```
YouTube Live vs Recorded Video
```

## invoke()

â†’ Recorded video (watch after completion)

## stream()

â†’ Live broadcast (watch while happening)

---

# ğŸ“Š When to Use Streaming

Use streaming when:

âœ” Graph takes time

âœ” Using LLMs

âœ” Building chat apps

âœ” Need transparency

âœ” Debugging

âœ” Real-time UX required

---

# âŒ When Not Needed

Avoid streaming when:

* Very small graphs
* Single fast computation
* No UI interaction needed

---

# ğŸ§± Complete Minimal Example (Copy-Paste Ready)

```python
from typing import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    message: str


def step1(state: State):
    return {"message": state["message"] + " â†’ step1"}


def step2(state: State):
    return {"message": state["message"] + " â†’ step2"}


graph = StateGraph(State)

graph.add_node("step1", step1)
graph.add_node("step2", step2)

graph.add_edge(START, "step1")
graph.add_edge("step1", "step2")
graph.add_edge("step2", END)

app = graph.compile()

for event in app.stream({"message": "start"}):
    print(event)
```

---

# â­ Key Takeaways

* Streaming = real-time graph execution
* Uses `graph.stream()`
* Returns generator of events
* Best for LLM apps and agents
* Enables live UI updates
* Improves debugging

---

# âœ… Summary

**LangGraph Streaming** allows graphs to produce outputs progressively instead of waiting for full completion. It is essential for building interactive AI systems, agent workflows, and real-time applications.

It transforms graph execution from:

```
batch execution â†’ event-driven execution
```

---

# ğŸ“š End of README

---
